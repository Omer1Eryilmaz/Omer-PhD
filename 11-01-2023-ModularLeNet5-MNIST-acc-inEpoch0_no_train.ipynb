{"cells":[{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["0 0.9850091934204102\n","1 0.9748775362968445\n","2 0.9624589681625366\n","Epoch 0  acc 0.075 (%):\n","Epoch 0  acc 0.075 (%):\n","Epoch 0  acc 0.075 (%):\n","Epoch 0  acc 0.076 (%):\n","Epoch 0  acc 0.076 (%):\n","Epoch 0  acc 0.077 (%):\n","Epoch 0  acc 0.077 (%):\n","Epoch 0  acc 0.078 (%):\n","Epoch 0  acc 0.078 (%):\n","Epoch 0  acc 0.079 (%):\n","Epoch 0  acc 0.079 (%):\n","Epoch 0  acc 0.079 (%):\n","Epoch 0  acc 0.080 (%):\n","Epoch 0  acc 0.080 (%):\n","Epoch 0  acc 0.080 (%):\n","Epoch 0  acc 0.080 (%):\n","Epoch 0  acc 0.081 (%):\n","Epoch 0  acc 0.081 (%):\n","Epoch 0  acc 0.081 (%):\n","Epoch 0  acc 0.081 (%):\n","Epoch 0  acc 0.081 (%):\n","Epoch 0  acc 0.082 (%):\n","Epoch 0  acc 0.082 (%):\n","Epoch 0  acc 0.082 (%):\n","Epoch 0  acc 0.082 (%):\n","Epoch 0  acc 0.083 (%):\n","Epoch 0  acc 0.083 (%):\n","Epoch 0  acc 0.084 (%):\n","Epoch 0  acc 0.084 (%):\n","Epoch 0  acc 0.084 (%):\n","Epoch 0  acc 0.084 (%):\n","Epoch 0  acc 0.084 (%):\n","Epoch 0  acc 0.084 (%):\n","Epoch 0  acc 0.084 (%):\n","Epoch 0  acc 0.085 (%):\n","Epoch 0  acc 0.085 (%):\n","Epoch 0  acc 0.085 (%):\n","Epoch 0  acc 0.085 (%):\n","Epoch 0  acc 0.085 (%):\n","Epoch 0  acc 0.085 (%):\n","Epoch 0  acc 0.085 (%):\n","Epoch 0  acc 0.086 (%):\n","Epoch 0  acc 0.086 (%):\n","Epoch 0  acc 0.086 (%):\n","Epoch 0  acc 0.086 (%):\n","Epoch 0  acc 0.086 (%):\n","Epoch 0  acc 0.086 (%):\n","Epoch 0  acc 0.087 (%):\n","Epoch 0  acc 0.087 (%):\n","Epoch 0  acc 0.087 (%):\n","Epoch 0  acc 0.087 (%):\n","Epoch 0  acc 0.087 (%):\n","Epoch 0  acc 0.088 (%):\n","Epoch 0  acc 0.088 (%):\n","Epoch 0  acc 0.088 (%):\n","Epoch 0  acc 0.088 (%):\n","Epoch 0  acc 0.089 (%):\n","Epoch 0  acc 0.089 (%):\n","Epoch 0  acc 0.089 (%):\n","Epoch 0  acc 0.089 (%):\n","Epoch 0  acc 0.089 (%):\n","Epoch 0  acc 0.089 (%):\n","Epoch 0  acc 0.089 (%):\n","Epoch 0  acc 0.089 (%):\n","Epoch 0  acc 0.089 (%):\n","Epoch 0  acc 0.089 (%):\n","Epoch 0  acc 0.089 (%):\n","Epoch 0  acc 0.089 (%):\n","Epoch 0  acc 0.090 (%):\n","Epoch 0  acc 0.090 (%):\n","Epoch 0  acc 0.090 (%):\n","Epoch 0  acc 0.090 (%):\n","Epoch 0  acc 0.090 (%):\n","Epoch 0  acc 0.090 (%):\n","Epoch 0  acc 0.090 (%):\n","Epoch 0  acc 0.090 (%):\n","Epoch 0  acc 0.090 (%):\n","Epoch 0  acc 0.090 (%):\n","Epoch 0  acc 0.090 (%):\n","Epoch 0  acc 0.091 (%):\n","Epoch 0  acc 0.091 (%):\n","Epoch 0  acc 0.091 (%):\n","Epoch 0  acc 0.092 (%):\n","Epoch 0  acc 0.092 (%):\n","Epoch 0  acc 0.092 (%):\n","Epoch 0  acc 0.092 (%):\n","Epoch 0  acc 0.092 (%):\n","Epoch 0  acc 0.092 (%):\n","Epoch 0  acc 0.092 (%):\n","Epoch 0  acc 0.093 (%):\n","Epoch 0  acc 0.093 (%):\n","Epoch 0  acc 0.093 (%):\n","Epoch 0  acc 0.093 (%):\n","Epoch 0  acc 0.093 (%):\n","Epoch 0  acc 0.093 (%):\n","Epoch 0  acc 0.093 (%):\n","Epoch 0  acc 0.093 (%):\n","Epoch 0  acc 0.093 (%):\n","Epoch 0  acc 0.093 (%):\n","Epoch 0  acc 0.094 (%):\n","Epoch 0  acc 0.094 (%):\n","Epoch 0  acc 0.094 (%):\n","Epoch 0  acc 0.094 (%):\n","Epoch 0  acc 0.094 (%):\n","Epoch 0  acc 0.094 (%):\n","Epoch 0  acc 0.094 (%):\n","Epoch 0  acc 0.094 (%):\n","Epoch 0  acc 0.094 (%):\n","Epoch 0  acc 0.094 (%):\n","Epoch 0  acc 0.095 (%):\n","Epoch 0  acc 0.095 (%):\n","Epoch 0  acc 0.095 (%):\n","Epoch 0  acc 0.095 (%):\n","Epoch 0  acc 0.095 (%):\n","Epoch 0  acc 0.095 (%):\n","Epoch 0  acc 0.095 (%):\n","Epoch 0  acc 0.095 (%):\n","Epoch 0  acc 0.095 (%):\n","Epoch 0  acc 0.095 (%):\n","Epoch 0  acc 0.096 (%):\n","Epoch 0  acc 0.096 (%):\n","Epoch 0  acc 0.096 (%):\n","Epoch 0  acc 0.096 (%):\n","Epoch 0  acc 0.097 (%):\n","Epoch 0  acc 0.097 (%):\n","Epoch 0  acc 0.097 (%):\n","Epoch 0  acc 0.097 (%):\n","Epoch 0  acc 0.098 (%):\n","Epoch 0  acc 0.098 (%):\n","Epoch 0  acc 0.099 (%):\n","Epoch 0  acc 0.099 (%):\n","Epoch 0  acc 0.099 (%):\n","Epoch 0  acc 0.099 (%):\n","Epoch 0  acc 0.099 (%):\n","Epoch 0  acc 0.099 (%):\n","Epoch 0  acc 0.099 (%):\n","Epoch 0  acc 0.099 (%):\n","Epoch 0  acc 0.099 (%):\n","Epoch 0  acc 0.100 (%):\n","Epoch 0  acc 0.100 (%):\n","Epoch 0  acc 0.100 (%):\n","Epoch 0  acc 0.100 (%):\n","Epoch 0  acc 0.100 (%):\n","Epoch 0  acc 0.100 (%):\n","Epoch 0  acc 0.101 (%):\n","Epoch 0  acc 0.101 (%):\n","Epoch 0  acc 0.101 (%):\n","Epoch 0  acc 0.101 (%):\n","Epoch 0  acc 0.101 (%):\n","Epoch 0  acc 0.102 (%):\n","Epoch 0  acc 0.102 (%):\n","Epoch 0  acc 0.102 (%):\n","Epoch 0  acc 0.102 (%):\n","Epoch 0  acc 0.103 (%):\n","Epoch 0  acc 0.103 (%):\n","Epoch 0  acc 0.103 (%):\n","Epoch 0  acc 0.104 (%):\n","Epoch 0  acc 0.104 (%):\n","Epoch 0  acc 0.104 (%):\n","Epoch 0  acc 0.105 (%):\n","Epoch 0  acc 0.105 (%):\n","Epoch 0  acc 0.105 (%):\n","Epoch 0  acc 0.106 (%):\n","Epoch 0  acc 0.106 (%):\n","Epoch 0  acc 0.106 (%):\n","Epoch 0  acc 0.107 (%):\n","Epoch 0  acc 0.107 (%):\n","Epoch 0  acc 0.107 (%):\n","Epoch 0  acc 0.108 (%):\n","Epoch 0  acc 0.108 (%):\n","Epoch 0  acc 0.108 (%):\n","Epoch 0  acc 0.109 (%):\n","Epoch 0  acc 0.109 (%):\n","Epoch 0  acc 0.109 (%):\n","Epoch 0  acc 0.110 (%):\n","Epoch 0  acc 0.110 (%):\n","Epoch 0  acc 0.111 (%):\n","Epoch 0  acc 0.111 (%):\n","Epoch 0  acc 0.112 (%):\n","Epoch 0  acc 0.112 (%):\n","Epoch 0  acc 0.113 (%):\n","Epoch 0  acc 0.113 (%):\n","Epoch 0  acc 0.113 (%):\n","Epoch 0  acc 0.114 (%):\n","Epoch 0  acc 0.114 (%):\n","Epoch 0  acc 0.114 (%):\n","Epoch 0  acc 0.114 (%):\n","Epoch 0  acc 0.115 (%):\n","Epoch 0  acc 0.115 (%):\n","Epoch 0  acc 0.116 (%):\n","Epoch 0  acc 0.116 (%):\n","Epoch 0  acc 0.116 (%):\n","Epoch 0  acc 0.116 (%):\n","Epoch 0  acc 0.117 (%):\n","Epoch 0  acc 0.117 (%):\n","Epoch 0  acc 0.118 (%):\n","Epoch 0  acc 0.118 (%):\n","Epoch 0  acc 0.118 (%):\n","Epoch 0  acc 0.119 (%):\n","Epoch 0  acc 0.120 (%):\n","Epoch 0  acc 0.120 (%):\n","Epoch 0  acc 0.121 (%):\n","Epoch 0  acc 0.121 (%):\n","Epoch 0  acc 0.122 (%):\n","Epoch 0  acc 0.122 (%):\n","Epoch 0  acc 0.123 (%):\n","Epoch 0  acc 0.123 (%):\n","Epoch 0  acc 0.124 (%):\n","Epoch 0  acc 0.124 (%):\n","Epoch 0  acc 0.124 (%):\n","Epoch 0  acc 0.125 (%):\n","Epoch 0  acc 0.126 (%):\n","Epoch 0  acc 0.126 (%):\n","Epoch 0  acc 0.126 (%):\n","Epoch 0  acc 0.127 (%):\n","Epoch 0  acc 0.127 (%):\n","Epoch 0  acc 0.127 (%):\n","Epoch 0  acc 0.128 (%):\n","Epoch 0  acc 0.128 (%):\n","Epoch 0  acc 0.128 (%):\n","Epoch 0  acc 0.128 (%):\n","Epoch 0  acc 0.129 (%):\n","Epoch 0  acc 0.129 (%):\n","Epoch 0  acc 0.129 (%):\n","Epoch 0  acc 0.130 (%):\n","Epoch 0  acc 0.131 (%):\n","Epoch 0  acc 0.131 (%):\n","Epoch 0  acc 0.132 (%):\n","Epoch 0  acc 0.132 (%):\n","Epoch 0  acc 0.133 (%):\n","Epoch 0  acc 0.134 (%):\n","Epoch 0  acc 0.134 (%):\n","Epoch 0  acc 0.135 (%):\n","Epoch 0  acc 0.136 (%):\n","Epoch 0  acc 0.136 (%):\n","Epoch 0  acc 0.136 (%):\n","Epoch 0  acc 0.137 (%):\n","Epoch 0  acc 0.137 (%):\n","Epoch 0  acc 0.138 (%):\n","Epoch 0  acc 0.138 (%):\n","Epoch 0  acc 0.139 (%):\n","Epoch 0  acc 0.139 (%):\n","Epoch 0  acc 0.140 (%):\n","Epoch 0  acc 0.141 (%):\n","Epoch 0  acc 0.141 (%):\n","Epoch 0  acc 0.141 (%):\n","Epoch 0  acc 0.142 (%):\n","Epoch 0  acc 0.142 (%):\n","Epoch 0  acc 0.143 (%):\n","Epoch 0  acc 0.143 (%):\n","Epoch 0  acc 0.144 (%):\n","Epoch 0  acc 0.144 (%):\n","Epoch 0  acc 0.145 (%):\n","Epoch 0  acc 0.146 (%):\n","Epoch 0  acc 0.146 (%):\n","Epoch 0  acc 0.146 (%):\n","Epoch 0  acc 0.147 (%):\n","Epoch 0  acc 0.148 (%):\n","Epoch 0  acc 0.149 (%):\n","Epoch 0  acc 0.149 (%):\n","Epoch 0  acc 0.150 (%):\n","Epoch 0  acc 0.151 (%):\n","Epoch 0  acc 0.151 (%):\n","Epoch 0  acc 0.152 (%):\n","Epoch 0  acc 0.153 (%):\n","Epoch 0  acc 0.154 (%):\n","Epoch 0  acc 0.155 (%):\n","Epoch 0  acc 0.155 (%):\n","Epoch 0  acc 0.156 (%):\n","Epoch 0  acc 0.157 (%):\n","Epoch 0  acc 0.158 (%):\n","Epoch 0  acc 0.159 (%):\n","Epoch 0  acc 0.160 (%):\n","Epoch 0  acc 0.160 (%):\n","Epoch 0  acc 0.161 (%):\n","Epoch 0  acc 0.162 (%):\n","Epoch 0  acc 0.162 (%):\n","Epoch 0  acc 0.162 (%):\n","Epoch 0  acc 0.163 (%):\n","Epoch 0  acc 0.164 (%):\n","Epoch 0  acc 0.165 (%):\n","Epoch 0  acc 0.165 (%):\n","Epoch 0  acc 0.166 (%):\n","Epoch 0  acc 0.167 (%):\n","Epoch 0  acc 0.168 (%):\n","Epoch 0  acc 0.169 (%):\n","Epoch 0  acc 0.171 (%):\n","Epoch 0  acc 0.172 (%):\n","Epoch 0  acc 0.172 (%):\n","Epoch 0  acc 0.173 (%):\n","Epoch 0  acc 0.174 (%):\n","Epoch 0  acc 0.175 (%):\n","Epoch 0  acc 0.177 (%):\n","Epoch 0  acc 0.178 (%):\n","Epoch 0  acc 0.180 (%):\n","Epoch 0  acc 0.181 (%):\n","Epoch 0  acc 0.182 (%):\n","Epoch 0  acc 0.183 (%):\n","Epoch 0  acc 0.185 (%):\n","Epoch 0  acc 0.187 (%):\n"]}],"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","import datetime\n","import torch\n","from torch import nn, optim\n","from torchvision import datasets, transforms\n","import torch.nn.functional as F\n","\n","device = (\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Use GPU or CPU for training\n","\n","transform = transforms.Compose([\n","    transforms.Resize((32, 32)),\n","    transforms.ToTensor()\n","])\n","\n","train_set = datasets.FashionMNIST('DATA_MNIST/', download=True, train=True, transform=transform)\n","trainloader = torch.utils.data.DataLoader(train_set, batch_size=200, shuffle=True)\n","# Since the dataset is already downloaded, download = False. \n","# If train=True, it creates dataset from train-images-idx3-ubyte, otherwise from t10k-images-idx3-ubyte.\n","test_set = datasets.FashionMNIST('DATA_MNIST/', download=False, train=False, transform=transform)\n","# shuffle (bool, optional) â€“ set to True to have the data reshuffled at every epoch (default: False). \n","testloader = torch.utils.data.DataLoader(test_set, batch_size=200, shuffle=True) #if accuracy is bad convert shuffle to False?\n","\n","train_data_size = len(train_set)\n","test_data_size = len(test_set)\n","\n","def tanh_norm(inputs):\n","    z = torch.tanh(inputs)\n","    znorm = z / (torch.sqrt(torch.sum(torch.pow(torch.abs(z), 2), axis=1, keepdims=True)))\n","    return znorm\n","\n","\n","InputModule = nn.Sequential(\n","    nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5, stride=1),\n","    nn.Tanh(),\n","    nn.MaxPool2d(kernel_size=2, stride=2, padding=0),\n","    nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, stride=1),\n","    nn.Tanh(),\n","    nn.MaxPool2d(kernel_size=2, stride=2, padding=0),\n","    nn.Conv2d(in_channels=16, out_channels=120, kernel_size=5, stride=1),\n","    nn.Tanh(),\n","    nn.Flatten(),\n","    nn.Linear(in_features=120, out_features=84)\n",").to(device)\n","\n","OutputModule = nn.Sequential(\n","    nn.Linear(in_features=84, out_features=10),\n",").to(device)\n","def forwardPass_InputModule(x):\n","    x = InputModule(x)\n","    return x\n","\n","\n","def forwardPass_OutputModule(x):\n","    x = tanh_norm(x)\n","    x = OutputModule(x)\n","    # x = F.softmax(x, dim=1) ***important change***\n","    return x\n","\n","\n","def SRS_Loss(x, y, num_classes):\n","    def k_mtrx(x0, x1):\n","        return torch.matmul(tanh_norm(x0), tanh_norm(x1).T)\n","\n","    def map_input(x):\n","        return torch.where(\n","            torch.eye(len(x)).to(x.device) == 1,\n","            torch.tensor(-float(\"inf\"), dtype=torch.float32, device=x.device),\n","            k_mtrx(x, x)\n","        )\n","\n","    def one_hot_encode(target, n_classes):\n","\n","        if target.ndim > 1:\n","            target = torch.squeeze(target)\n","        target_onehot = torch.zeros((target.shape[0], n_classes))\n","        # target_onehot[range(target.size(0)), target] = 1\n","        target_onehot[range(target.size(0)), target.type(torch.long)] = 1\n","        # target_onehot[range(target.shape[0]), target.type(torch.long)] = 1\n","        return target_onehot\n","\n","    def get_ideal_k_mtrx(target1, target2, n_classes):\n","\n","        k_min = -1.\n","        k_max = 1.\n","        if n_classes < 2:\n","            raise ValueError('You need at least 2 classes')\n","\n","        if len(target1.shape) > 2:\n","            raise ValueError('target1 has too many dimensions')\n","        if len(target2.shape) > 2:\n","            raise ValueError('target2 has too many dimensions')\n","\n","        if torch.max(target1) + 1 > n_classes:\n","            raise ValueError('target1 has at least one invalid entry')\n","        if torch.max(target2) + 1 > n_classes:\n","            raise ValueError('target2 has at least one invalid entry')\n","\n","        target1_onehot, target2_onehot = \\\n","            one_hot_encode(target1, n_classes).type(torch.float32), \\\n","            one_hot_encode(target2, n_classes).type(torch.float32)\n","        ideal = torch.matmul(target1_onehot, target2_onehot.T)\n","        if k_min != 0:  # for tanh activation function\n","            min_mask = torch.full(ideal.shape, k_min)\n","            ideal = torch.where(ideal == 0, min_mask, ideal)  # Same class indices are 1 others -1.\n","\n","        if k_max != 1:  # for ReLU activation function\n","            max_mask = torch.full_like(ideal, k_max)\n","            ideal = torch.where(ideal == 1, max_mask, ideal)  # Same class indices are MaxValue others 0.\n","        return ideal\n","\n","   \n","\n","    k_min = -1.\n","    xx = map_input(x)\n","    k_ideal = get_ideal_k_mtrx(y, y, num_classes)  # We are using phi(tanh) in the first module's training.\n","    return torch.mean(torch.exp(xx[k_ideal == k_min])) # loss is possitive different from the paper.\n","\n","######################### Train the input module #########################\n","optimizer = torch.optim.Adam(params=InputModule.parameters(), lr=1e-3)\n","\n","num_classes = 10\n","n_epochs =  3\n","\n","# Train Input module\n","for epoch in range(n_epochs):\n","    for batch in trainloader:\n","        inputs, targets = batch\n","        inputs = inputs.to(device)\n","        targets = targets.to(device)\n","        optimizer.zero_grad()\n","        loss_fn = SRS_Loss(forwardPass_InputModule(inputs), targets, num_classes)\n","        loss_fn.backward()\n","        optimizer.step()\n","\n","    if (epoch % 1) == 0:\n","        print(epoch, loss_fn.item())\n","        net_repr = forwardPass_InputModule(inputs).detach().cpu()\n","        net_repr = tanh_norm(net_repr).numpy()\n","\n","        # fig = plt.figure()\n","        # plt.title(\"Input Module Epoch{}\".format(epoch))\n","        # ax = fig.add_subplot(projection='3d')\n","        # ax.scatter(net_repr[:, 0], net_repr[:, 1], net_repr[:, 2], c=targets)\n","        # plt.show()\n","\n","# Train Output Module\n","OutputModule_optimizer = torch.optim.Adam(params=OutputModule.parameters(), lr=1e-3)\n","OutputModule_loss_fn = torch.nn.CrossEntropyLoss()\n","# net = nn.Sequential(InputModule, OutputModule)\n","#stop gradient\n","for param in InputModule.parameters():\n","    param.requires_grad = False\n","n_epochs =  1\n","\n","\n","# initilize Output module parameters\n","for param in OutputModule.parameters():\n","    #nn.init.uniform_(param)\n","    nn.init.normal_(param)\n","\n","\n","\n","# for epoch in range(n_epochs):\n","\n","#     InputModule.train()\n","#     OutputModule.train()\n","    \n","#     for batch in trainloader:\n","#         inputs, targets = batch\n","#         inputs = inputs.to(device)\n","#         targets = targets.to(device)\n","#         OutputModule_optimizer.zero_grad()\n","#         InputForOutputModule = forwardPass_InputModule(inputs)\n","#         # print(forwardPass_OutputModule(InputForOutputModule).shape)\n","#         # print()\n","#         loss_fn = OutputModule_loss_fn(forwardPass_OutputModule(InputForOutputModule), targets)\n","#         loss_fn.backward()\n","#         OutputModule_optimizer.step()\n","#         preds = []\n","#         labels = []\n","#         with torch.no_grad():\n","#             for batch in testloader:\n","#                 inputs, targets = batch\n","#                 inputs = inputs.to(device)\n","#                 pred = forwardPass_InputModule(inputs)\n","#                 pred = forwardPass_OutputModule(pred)\n","#                 pred = F.softmax(pred, dim=1) # Why don't we put this in the OutMod TRAINING?\n","#                 _, pred = torch.max(pred, dim=1)\n","#                 preds += pred.detach().cpu().numpy().tolist()\n","#                 labels += targets.detach().cpu().numpy().tolist()\n","#             pred = np.array(preds)# disari al\n","#             targets = np.array(labels)\n","#             print(\"Epoch {}  acc {:.3f} (%):\".format(epoch, np.mean(pred == targets)))\n","    \n","# get final accuracy\n","InputModule.eval()\n","OutputModule.eval()\n","preds = []\n","labels = []\n","with torch.no_grad():\n","    for batch in testloader:\n","        inputs, targets = batch\n","        inputs = inputs.to(device)\n","        pred = forwardPass_InputModule(inputs)\n","        pred = forwardPass_OutputModule(pred)\n","        pred = F.softmax(pred, dim=1) # Why don't we put this in the OutMod TRAINING?\n","        _, pred = torch.max(pred, dim=1)\n","        preds += pred.detach().cpu().numpy().tolist()\n","        labels += targets.detach().cpu().numpy().tolist()\n","    pred = np.array(preds)# disari al\n","    targets = np.array(labels)\n","    print(\"Epoch {}  acc {:.3f} (%):\".format(epoch, np.mean(pred == targets)))"]}],"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":3},"orig_nbformat":4},"nbformat":4,"nbformat_minor":2}
